{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer learning in traffic flow prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load a configuration\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import time\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    merge,\n",
    "    Dense,\n",
    "    Reshape\n",
    ")\n",
    "from copy import copy\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model as plot\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "# from keras.layers import Dense\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "import math\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "        \n",
    "class config():\n",
    "    def __init__(self):\n",
    "        self.DATAPATH='../Data/CrowdFlows'\n",
    "        self.nb_epoch = 500  # number of epoch at training stage\n",
    "        self.nb_epoch_cont = 100  # number of epoch at training (cont) stage\n",
    "        self.batch_size = 32  # batch size\n",
    "        self.lr = 0.0002  # learning rate\n",
    "        self.nb_flow = 2  # there are two types of flows: new-flow and end-flow\n",
    "        self.meta_data = True\n",
    "        self.holiday_data = False\n",
    "        self.meteorol_data = False\n",
    "    def NYC(self):\n",
    "        self.T = 24  # number of time intervals in one day\n",
    "        self.len_closeness = 3  # length of closeness dependent sequence\n",
    "        self.len_period = 4  # length of peroid dependent sequence\n",
    "        self.len_trend = 4  # length of trend dependent sequence\n",
    "        self.nb_residual_unit = 4   # number of residual units\n",
    "\n",
    "        # divide data into two subsets: Train & Test, of which the test set is the\n",
    "        # last 10 days\n",
    "        self.days_test = 10\n",
    "        self.len_test = self.T * self.days_test\n",
    "        self.map_height, self.map_width = 16, 8  # grid size\n",
    "        # For NYC Bike data, there are 81 available grid-based areas, each of\n",
    "        # which includes at least ONE bike station. Therefore, we modify the final\n",
    "        # RMSE by multiplying the following factor (i.e., factor).\n",
    "        self.nb_area = 81\n",
    "        self.m_factor = math.sqrt(1. * self.map_height * self.map_width / self.nb_area)\n",
    "        self.DIR = 'BikeNYC'\n",
    "        self.file = ['NYC14_M16x8_T60_NewEnd.h5']\n",
    "        return self  \n",
    "    def BJ(self):\n",
    "        self.T = 48  # number of time intervals in one day\n",
    "        self.len_closeness = 3  # length of closeness dependent sequence\n",
    "        self.len_period = 1  # length of peroid dependent sequence\n",
    "        self.len_trend = 1  # length of trend dependent sequence\n",
    "        self.nb_residual_unit = 4\n",
    "        self.days_test = 7 * 4\n",
    "        self.len_test = self.T * self.days_test\n",
    "        self.map_height, self.map_width = 32, 32  # grid size\n",
    "        self.m_factor = 1\n",
    "        self.DIR = 'TaxiBJ'\n",
    "        self.file = ['BJ13_M32x32_T30_InOut.h5','BJ14_M32x32_T30_InOut.h5','BJ15_M32x32_T30_InOut.h5','BJ16_M32x32_T30_InOut.h5']\n",
    "        self.holiday_data = True\n",
    "        self.holiday_file = 'BJ_Holiday.txt'\n",
    "        self.meteorol_data = True\n",
    "        self.meteorol_file = 'BJ_Meteorology.h5'\n",
    "        return self\n",
    "\n",
    "print(\"load a configuration\")\n",
    "CONFIG = config().BJ()\n",
    "CACHEDATA = True\n",
    "path_result = 'RET'\n",
    "path_model = 'MODEL'\n",
    "try:\n",
    "    K.set_image_data_format(\"channels_first\")\n",
    "except Exception,e:\n",
    "    pass\n",
    "\n",
    "if os.path.isdir(path_result) is False:\n",
    "    os.mkdir(path_result)\n",
    "if os.path.isdir(path_model) is False:\n",
    "    os.mkdir(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string2timestamp(strings, T=48):\n",
    "    timestamps = []\n",
    "\n",
    "    time_per_slot = 24.0 / T\n",
    "    num_per_T = T // 24\n",
    "    for t in strings:\n",
    "        year, month, day, slot = int(t[:4]), int(t[4:6]), int(t[6:8]), int(t[8:])-1\n",
    "        timestamps.append(pd.Timestamp(datetime(year, month, day, hour=int(slot * time_per_slot), minute=(slot % num_per_T) * int(60.0 * time_per_slot))))\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "def load_stdata(fname):\n",
    "    f = h5py.File(fname, 'r')\n",
    "    data = f['data'].value\n",
    "    timestamps = f['date'].value\n",
    "    f.close()\n",
    "    return data, timestamps\n",
    "\n",
    "def load_holiday(timeslots, fname):\n",
    "    f = open(fname, 'r')\n",
    "    holidays = f.readlines()\n",
    "    holidays = set([h.strip() for h in holidays])\n",
    "    H = np.zeros(len(timeslots))\n",
    "    for i, slot in enumerate(timeslots):\n",
    "        if slot[:8] in holidays:\n",
    "            H[i] = 1\n",
    "    print(H.sum())\n",
    "    # print(timeslots[H==1])\n",
    "    return H[:, None]\n",
    "\n",
    "def load_meteorol(timeslots, fname):\n",
    "    '''\n",
    "    timeslots: the predicted timeslots\n",
    "    In real-world, we dont have the meteorol data in the predicted timeslot, instead, we use the meteoral at previous timeslots, i.e., slot = predicted_slot - timeslot (you can use predicted meteorol data as well)\n",
    "    '''\n",
    "    f = h5py.File(fname, 'r')\n",
    "    Timeslot = f['date'].value\n",
    "    WindSpeed = f['WindSpeed'].value\n",
    "    Weather = f['Weather'].value\n",
    "    Temperature = f['Temperature'].value\n",
    "    f.close()\n",
    "\n",
    "    M = dict()  # map timeslot to index\n",
    "    for i, slot in enumerate(Timeslot):\n",
    "        M[slot] = i\n",
    "\n",
    "    WS = []  # WindSpeed\n",
    "    WR = []  # Weather\n",
    "    TE = []  # Temperature\n",
    "    for slot in timeslots:\n",
    "        predicted_id = M[slot]\n",
    "        cur_id = predicted_id - 1\n",
    "        WS.append(WindSpeed[cur_id])\n",
    "        WR.append(Weather[cur_id])\n",
    "        TE.append(Temperature[cur_id])\n",
    "\n",
    "    WS = np.asarray(WS)\n",
    "    WR = np.asarray(WR)\n",
    "    TE = np.asarray(TE)\n",
    "\n",
    "    # 0-1 scale\n",
    "    WS = 1. * (WS - WS.min()) / (WS.max() - WS.min())\n",
    "    TE = 1. * (TE - TE.min()) / (TE.max() - TE.min())\n",
    "\n",
    "    print(\"shape: \", WS.shape, WR.shape, TE.shape)\n",
    "\n",
    "    # concatenate all these attributes\n",
    "    merge_data = np.hstack([WR, WS[:, None], TE[:, None]])\n",
    "\n",
    "    # print('meger shape:', merge_data.shape)\n",
    "    return merge_data\n",
    "\n",
    "def remove_incomplete_days(data, timestamps, T=48):\n",
    "    # remove a certain day which has not 48 timestamps\n",
    "    days = []  # available days: some day only contain some seqs\n",
    "    days_incomplete = []\n",
    "    i = 0\n",
    "    while i < len(timestamps):\n",
    "        if int(timestamps[i][8:]) != 1:\n",
    "            i += 1\n",
    "        elif i+T-1 < len(timestamps) and int(timestamps[i+T-1][8:]) == T:\n",
    "            days.append(timestamps[i][:8])\n",
    "            i += T\n",
    "        else:\n",
    "            days_incomplete.append(timestamps[i][:8])\n",
    "            i += 1\n",
    "    print(\"incomplete days: \", days_incomplete)\n",
    "    days = set(days)\n",
    "    idx = []\n",
    "    for i, t in enumerate(timestamps):\n",
    "        if t[:8] in days:\n",
    "            idx.append(i)\n",
    "\n",
    "    data = data[idx]\n",
    "    timestamps = [timestamps[i] for i in idx]\n",
    "    return data, timestamps\n",
    "\n",
    "def stat(fname):\n",
    "    def get_nb_timeslot(f):\n",
    "        s = f['date'][0]\n",
    "        e = f['date'][-1]\n",
    "        year, month, day = map(int, [s[:4], s[4:6], s[6:8]])\n",
    "        ts = time.strptime(\"%04i-%02i-%02i\" % (year, month, day), \"%Y-%m-%d\")\n",
    "        year, month, day = map(int, [e[:4], e[4:6], e[6:8]])\n",
    "        te = time.strptime(\"%04i-%02i-%02i\" % (year, month, day), \"%Y-%m-%d\")\n",
    "        nb_timeslot = (time.mktime(te) - time.mktime(ts)) / (0.5 * 3600) + 48\n",
    "        ts_str, te_str = time.strftime(\"%Y-%m-%d\", ts), time.strftime(\"%Y-%m-%d\", te)\n",
    "        return nb_timeslot, ts_str, te_str\n",
    "\n",
    "    with h5py.File(fname) as f:\n",
    "        nb_timeslot, ts_str, te_str = get_nb_timeslot(f)\n",
    "        nb_day = int(nb_timeslot / 48)\n",
    "        mmax = f['data'].value.max()\n",
    "        mmin = f['data'].value.min()\n",
    "        stat = '=' * 5 + 'stat' + '=' * 5 + '\\n' + \\\n",
    "               'data shape: %s\\n' % str(f['data'].shape) + \\\n",
    "               '# of days: %i, from %s to %s\\n' % (nb_day, ts_str, te_str) + \\\n",
    "               '# of timeslots: %i\\n' % int(nb_timeslot) + \\\n",
    "               '# of timeslots (available): %i\\n' % f['date'].shape[0] + \\\n",
    "               'missing ratio of timeslots: %.1f%%\\n' % ((1. - float(f['date'].shape[0] / nb_timeslot)) * 100) + \\\n",
    "               'max: %.3f, min: %.3f\\n' % (mmax, mmin) + \\\n",
    "               '=' * 5 + 'stat' + '=' * 5\n",
    "        print(stat)\n",
    "        \n",
    "def timestamp2vec(timestamps):\n",
    "    # tm_wday range [0, 6], Monday is 0\n",
    "    # vec = [time.strptime(str(t[:8], encoding='utf-8'), '%Y%m%d').tm_wday for t in timestamps]  # python3\n",
    "    vec = [time.strptime(t[:8], '%Y%m%d').tm_wday for t in timestamps]  # python2\n",
    "    ret = []\n",
    "    for i in vec:\n",
    "        v = [0 for _ in range(7)]\n",
    "        v[i] = 1\n",
    "        if i >= 5: \n",
    "            v.append(0)  # weekend\n",
    "        else:\n",
    "            v.append(1)  # weekday\n",
    "        ret.append(v)\n",
    "    return np.asarray(ret)\n",
    "\n",
    "def read_cache(fname):\n",
    "    mmn = pickle.load(open('preprocessing.pkl', 'rb'))\n",
    "\n",
    "    f = h5py.File(fname, 'r')\n",
    "    num = int(f['num'].value)\n",
    "    X_train, Y_train, X_test, Y_test = [], [], [], []\n",
    "    for i in xrange(num):\n",
    "        X_train.append(f['X_train_%i' % i].value)\n",
    "        X_test.append(f['X_test_%i' % i].value)\n",
    "    Y_train = f['Y_train'].value\n",
    "    Y_test = f['Y_test'].value\n",
    "    external_dim = f['external_dim'].value\n",
    "    timestamp_train = f['T_train'].value\n",
    "    timestamp_test = f['T_test'].value\n",
    "    f.close()\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, mmn, external_dim, timestamp_train, timestamp_test\n",
    "\n",
    "\n",
    "def cache(fname, X_train, Y_train, X_test, Y_test, external_dim, timestamp_train, timestamp_test):\n",
    "    h5 = h5py.File(fname, 'w')\n",
    "    h5.create_dataset('num', data=len(X_train))\n",
    "\n",
    "    for i, data in enumerate(X_train):\n",
    "        h5.create_dataset('X_train_%i' % i, data=data)\n",
    "    # for i, data in enumerate(Y_train):\n",
    "    for i, data in enumerate(X_test):\n",
    "        h5.create_dataset('X_test_%i' % i, data=data)\n",
    "    h5.create_dataset('Y_train', data=Y_train)\n",
    "    h5.create_dataset('Y_test', data=Y_test)\n",
    "    external_dim = -1 if external_dim is None else int(external_dim)\n",
    "    h5.create_dataset('external_dim', data=external_dim)\n",
    "    h5.create_dataset('T_train', data=timestamp_train)\n",
    "    h5.create_dataset('T_test', data=timestamp_test)\n",
    "    h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalization(object):\n",
    "    '''MinMax Normalization --> [-1, 1]\n",
    "       x = (x - min) / (max - min).\n",
    "       x = x * 2 - 1\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        self._min = X.min()\n",
    "        self._max = X.max()\n",
    "        print(\"min:\", self._min, \"max:\", self._max)\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = 1. * (X - self._min) / (self._max - self._min)\n",
    "        X = X * 2. - 1.\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X = (X + 1.) / 2.\n",
    "        X = 1. * X * (self._max - self._min) + self._min\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a data structure for ST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STMatrix(object):\n",
    "    \"\"\"docstring for STMatrix\"\"\"\n",
    "\n",
    "    def __init__(self, data, timestamps, T=48, CheckComplete=True):\n",
    "        super(STMatrix, self).__init__()\n",
    "        assert len(data) == len(timestamps)\n",
    "        self.data = data\n",
    "        self.timestamps = timestamps\n",
    "        self.T = T\n",
    "        self.pd_timestamps = string2timestamp(timestamps, T=self.T)\n",
    "        if CheckComplete:\n",
    "            self.check_complete()\n",
    "        # index\n",
    "        self.make_index()\n",
    "\n",
    "    def make_index(self):\n",
    "        self.get_index = dict()\n",
    "        for i, ts in enumerate(self.pd_timestamps):\n",
    "            self.get_index[ts] = i\n",
    "\n",
    "    def check_complete(self):\n",
    "        missing_timestamps = []\n",
    "        offset = pd.DateOffset(minutes=24 * 60 // self.T)\n",
    "        pd_timestamps = self.pd_timestamps\n",
    "        i = 1\n",
    "        while i < len(pd_timestamps):\n",
    "            if pd_timestamps[i-1] + offset != pd_timestamps[i]:\n",
    "                missing_timestamps.append(\"(%s -- %s)\" % (pd_timestamps[i-1], pd_timestamps[i]))\n",
    "            i += 1\n",
    "        for v in missing_timestamps:\n",
    "            print(v)\n",
    "        assert len(missing_timestamps) == 0\n",
    "\n",
    "    def get_matrix(self, timestamp):\n",
    "        return self.data[self.get_index[timestamp]]\n",
    "\n",
    "    def save(self, fname):\n",
    "        pass\n",
    "\n",
    "    def check_it(self, depends):\n",
    "        for d in depends:\n",
    "            if d not in self.get_index.keys():\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def create_dataset(self, len_closeness=3, len_trend=3, TrendInterval=7, len_period=3, PeriodInterval=1):\n",
    "        \"\"\"current version\n",
    "        \"\"\"\n",
    "        # offset_frame: one-frame offset\n",
    "        offset_frame = pd.DateOffset(minutes=24 * 60 // self.T)\n",
    "        XC = []\n",
    "        XP = []\n",
    "        XT = []\n",
    "        Y = []\n",
    "        timestamps_Y = []\n",
    "        depends = [range(1, len_closeness+1),\n",
    "                   [PeriodInterval * self.T * j for j in range(1, len_period+1)],\n",
    "                   [TrendInterval * self.T * j for j in range(1, len_trend+1)]]\n",
    "\n",
    "        i = max(self.T * TrendInterval * len_trend, self.T * PeriodInterval * len_period, len_closeness)\n",
    "        while i < len(self.pd_timestamps):\n",
    "            Flag = True\n",
    "            for depend in depends:\n",
    "                if Flag is False:\n",
    "                    break\n",
    "                Flag = self.check_it([self.pd_timestamps[i] - j * offset_frame for j in depend])\n",
    "\n",
    "            if Flag is False:\n",
    "                i += 1\n",
    "                continue\n",
    "            x_c = [self.get_matrix(self.pd_timestamps[i] - j * offset_frame) for j in depends[0]]\n",
    "            x_p = [self.get_matrix(self.pd_timestamps[i] - j * offset_frame) for j in depends[1]]\n",
    "            x_t = [self.get_matrix(self.pd_timestamps[i] - j * offset_frame) for j in depends[2]]\n",
    "            y = self.get_matrix(self.pd_timestamps[i])\n",
    "            if len_closeness > 0:\n",
    "                XC.append(np.vstack(x_c))\n",
    "            if len_period > 0:\n",
    "                XP.append(np.vstack(x_p))\n",
    "            if len_trend > 0:\n",
    "                XT.append(np.vstack(x_t))\n",
    "            Y.append(y)\n",
    "            timestamps_Y.append(self.timestamps[i])\n",
    "            i += 1\n",
    "        XC = np.asarray(XC)\n",
    "        XP = np.asarray(XP)\n",
    "        XT = np.asarray(XT)\n",
    "        Y = np.asarray(Y)\n",
    "        return XC, XP, XT, Y, timestamps_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define load data function\n",
    "return X_train, Y_train, X_test, Y_test, mmn, metadata_dim, timestamp_train, timestamp_test\n",
    "\n",
    "X_train:[XC_train,XP_train,XT_train,meta_train]\n",
    "X_test:[XC_test,XP_test,XT_test,meta_test]\n",
    "XCPT_train/test.shape:(# of seq, len of seq, 16, 8)\n",
    "meta:(# of seq, 8), [:7]:week of day; [7]:is weekday\n",
    "Y_train,Y_test:(#of seq, 2, 16, 8)\n",
    "\n",
    "mmn: minmax normalizer\n",
    "\n",
    "external_dim: 8\n",
    "timestamp_train, timestamp_test: [2014042901...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(T=48, nb_flow=2, len_closeness=None, len_period=None, len_trend=None,\n",
    "              len_test=None, preprocess_name='preprocessing.pkl',\n",
    "              meta_data=True, meteorol_data=True, holiday_data=True):\n",
    "    assert(len_closeness + len_period + len_trend > 0)\n",
    "    data_all = []\n",
    "    timestamps_all = list()\n",
    "    for f in CONFIG.file:\n",
    "        fname = os.path.join(CONFIG.DATAPATH, CONFIG.DIR, f)\n",
    "        print(\"file name: \", fname)\n",
    "        stat(fname)\n",
    "        data, timestamps = load_stdata(fname)\n",
    "        data, timestamps = remove_incomplete_days(data, timestamps, T)\n",
    "        data = data[:, :nb_flow]\n",
    "        data[data < 0] = 0.\n",
    "        data_all.append(data)\n",
    "        timestamps_all.append(timestamps)\n",
    "        print(\"\\n\")\n",
    "    data_train = np.vstack(copy(data_all))[:-len_test]\n",
    "\n",
    "    print('train_data shape: ', data_train.shape)\n",
    "    mmn = MinMaxNormalization()\n",
    "    mmn.fit(data_train)\n",
    "    data_all_mmn = [mmn.transform(d) for d in data_all]\n",
    "\n",
    "    fpkl = open('preprocessing.pkl', 'wb')\n",
    "    for obj in [mmn]:\n",
    "        pickle.dump(obj, fpkl)\n",
    "    fpkl.close()\n",
    "\n",
    "    #generate feature sequences\n",
    "    XC, XP, XT = [], [], []\n",
    "    Y = []\n",
    "    timestamps_Y = []\n",
    "    for data, timestamps in zip(data_all_mmn, timestamps_all):\n",
    "        # instance-based dataset --> sequences with format as (X, Y) where X is a sequence of images and Y is an image.\n",
    "        st = STMatrix(data, timestamps, T, CheckComplete=False)\n",
    "        _XC, _XP, _XT, _Y, _timestamps_Y = st.create_dataset(len_closeness=len_closeness, len_period=len_period, len_trend=len_trend)\n",
    "        XC.append(_XC)\n",
    "        XP.append(_XP)\n",
    "        XT.append(_XT)\n",
    "        Y.append(_Y)\n",
    "        timestamps_Y += _timestamps_Y\n",
    "    \n",
    "    # load meta feature\n",
    "    meta_feature = []\n",
    "    if CONFIG.meta_data:\n",
    "        time_feature = timestamp2vec(timestamps_Y)\n",
    "        meta_feature.append(time_feature)\n",
    "    if CONFIG.holiday_data:\n",
    "        holiday_feature = load_holiday(timestamps_Y,fname=os.path.join(CONFIG.DATAPATH, CONFIG.DIR, CONFIG.holiday_file))\n",
    "        meta_feature.append(holiday_feature)\n",
    "    if meteorol_data:\n",
    "        meteorol_feature = load_meteorol(timestamps_Y,fname=os.path.join(CONFIG.DATAPATH, CONFIG.DIR, CONFIG.meteorol_file))\n",
    "        meta_feature.append(meteorol_feature)\n",
    "   \n",
    "    meta_feature = np.hstack(meta_feature) if len(meta_feature) > 0 else np.asarray(meta_feature)\n",
    "    metadata_dim = meta_feature.shape[1] if len(meta_feature.shape) > 1 else None\n",
    "    if metadata_dim < 1:\n",
    "        metadata_dim = None\n",
    "    if meta_data and holiday_data and meteorol_data:\n",
    "        print('time feature:', time_feature.shape, 'holiday feature:', holiday_feature.shape,\n",
    "              'meteorol feature: ', meteorol_feature.shape, 'mete feature: ', meta_feature.shape)\n",
    "    \n",
    "        \n",
    "    XC = np.vstack(XC)\n",
    "    XP = np.vstack(XP)\n",
    "    XT = np.vstack(XT)\n",
    "    Y = np.vstack(Y)\n",
    "    print(\"XC shape: \", XC.shape, \"XP shape: \", XP.shape, \"XT shape: \", XT.shape, \"Y shape:\", Y.shape)\n",
    "    XC_train, XP_train, XT_train, Y_train = XC[:-len_test], XP[:-len_test], XT[:-len_test], Y[:-len_test]\n",
    "    XC_test, XP_test, XT_test, Y_test = XC[-len_test:], XP[-len_test:], XT[-len_test:], Y[-len_test:]\n",
    "    timestamp_train, timestamp_test = timestamps_Y[:-len_test], timestamps_Y[-len_test:]\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    for l, X_ in zip([len_closeness, len_period, len_trend], [XC_train, XP_train, XT_train]):\n",
    "        if l > 0:\n",
    "            X_train.append(X_)\n",
    "    for l, X_ in zip([len_closeness, len_period, len_trend], [XC_test, XP_test, XT_test]):\n",
    "        if l > 0:\n",
    "            X_test.append(X_)\n",
    "    print('train shape:', XC_train.shape, Y_train.shape, 'test shape: ', XC_test.shape, Y_test.shape)\n",
    "    \n",
    "    if metadata_dim is not None:\n",
    "        meta_feature_train, meta_feature_test = meta_feature[:-len_test], meta_feature[-len_test:]\n",
    "        X_train.append(meta_feature_train)\n",
    "        X_test.append(meta_feature_test)\n",
    "    for _X in X_train:\n",
    "        print(_X.shape, )\n",
    "    print()\n",
    "    for _X in X_test:\n",
    "        print(_X.shape, )\n",
    "    print()\n",
    "    return X_train, Y_train, X_test, Y_test, mmn, metadata_dim, timestamp_train, timestamp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "('file name: ', '../Data/CrowdFlows/TaxiBJ/BJ13_M32x32_T30_InOut.h5')\n",
      "=====stat=====\n",
      "data shape: (4888, 2, 32, 32)\n",
      "# of days: 121, from 2013-07-01 to 2013-10-29\n",
      "# of timeslots: 5808\n",
      "# of timeslots (available): 4888\n",
      "missing ratio of timeslots: 15.8%\n",
      "max: 1230.000, min: 0.000\n",
      "=====stat=====\n",
      "('incomplete days: ', ['20130926'])\n",
      "\n",
      "\n",
      "('file name: ', '../Data/CrowdFlows/TaxiBJ/BJ14_M32x32_T30_InOut.h5')\n",
      "=====stat=====\n",
      "data shape: (4780, 2, 32, 32)\n",
      "# of days: 119, from 2014-03-01 to 2014-06-27\n",
      "# of timeslots: 5712\n",
      "# of timeslots (available): 4780\n",
      "missing ratio of timeslots: 16.3%\n",
      "max: 1292.000, min: 0.000\n",
      "=====stat=====\n",
      "('incomplete days: ', ['20140304', '20140313', '20140323', '20140326', '20140401', '20140402', '20140409', '20140410', '20140412', '20140422', '20140501', '20140526', '20140618', '20140627'])\n",
      "\n",
      "\n",
      "('file name: ', '../Data/CrowdFlows/TaxiBJ/BJ15_M32x32_T30_InOut.h5')\n",
      "=====stat=====\n",
      "data shape: (5596, 2, 32, 32)\n",
      "# of days: 122, from 2015-03-01 to 2015-06-30\n",
      "# of timeslots: 5856\n",
      "# of timeslots (available): 5596\n",
      "missing ratio of timeslots: 4.4%\n",
      "max: 1274.000, min: 0.000\n",
      "=====stat=====\n",
      "('incomplete days: ', ['20150320', '20150404', '20150620'])\n",
      "\n",
      "\n",
      "('file name: ', '../Data/CrowdFlows/TaxiBJ/BJ16_M32x32_T30_InOut.h5')\n",
      "=====stat=====\n",
      "data shape: (7220, 2, 32, 32)\n",
      "# of days: 162, from 2015-11-01 to 2016-04-10\n",
      "# of timeslots: 7776\n",
      "# of timeslots (available): 7220\n",
      "missing ratio of timeslots: 7.2%\n",
      "max: 1250.000, min: 0.000\n",
      "=====stat=====\n",
      "('incomplete days: ', ['20151101', '20151103', '20151109', '20151110', '20151112', '20151121', '20151124', '20151128', '20151201', '20151203', '20151206', '20151210', '20151211', '20151212', '20151217', '20151227', '20151229', '20160111', '20160127', '20160323', '20160410'])\n",
      "\n",
      "\n",
      "('train_data shape: ', (20016, 2, 32, 32))\n",
      "('min:', 0.0, 'max:', 1292.0)\n",
      "1008.0\n",
      "('shape: ', (15072,), (15072, 17), (15072,))\n",
      "('time feature:', (15072, 8), 'holiday feature:', (15072, 1), 'meteorol feature: ', (15072, 19), 'mete feature: ', (15072, 28))\n",
      "('XC shape: ', (15072, 6, 32, 32), 'XP shape: ', (15072, 2, 32, 32), 'XT shape: ', (15072, 2, 32, 32), 'Y shape:', (15072, 2, 32, 32))\n",
      "('train shape:', (13728, 6, 32, 32), (13728, 2, 32, 32), 'test shape: ', (1344, 6, 32, 32), (1344, 2, 32, 32))\n",
      "((13728, 6, 32, 32),)\n",
      "((13728, 2, 32, 32),)\n",
      "((13728, 2, 32, 32),)\n",
      "((13728, 28),)\n",
      "()\n",
      "((1344, 6, 32, 32),)\n",
      "((1344, 2, 32, 32),)\n",
      "((1344, 2, 32, 32),)\n",
      "((1344, 28),)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(\"loading data...\")\n",
    "ts = time.time()\n",
    "fname = os.path.join(CONFIG.DATAPATH, 'CACHE', 'TaxiBJ_C{}_P{}_T{}.h5'.format(CONFIG.len_closeness, CONFIG.len_period, CONFIG.len_trend))\n",
    "if os.path.exists(fname) and CACHEDATA:\n",
    "    X_train, Y_train, X_test, Y_test, mmn, external_dim, timestamp_train, timestamp_test = read_cache(fname)\n",
    "    print(\"load %s successfully\" % fname)\n",
    "else:\n",
    "    X_train, Y_train, X_test, Y_test, mmn, external_dim, timestamp_train, timestamp_test = load_data(\\\n",
    "            T=CONFIG.T, nb_flow=CONFIG.nb_flow, len_closeness=CONFIG.len_closeness, len_period=CONFIG.len_period, \\\n",
    "            len_trend=CONFIG.len_trend, len_test=CONFIG.len_test, preprocess_name='preprocessing.pkl', \\\n",
    "            meta_data=CONFIG.meta_data, meteorol_data=CONFIG.meteorol_data, holiday_data=CONFIG.holiday_data)\n",
    "    if CACHEDATA:\n",
    "        cache(fname, X_train, Y_train, X_test, Y_test, external_dim, timestamp_train, timestamp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        # self.output_dim = output_dim\n",
    "        super(iLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        initial_weight_value = np.random.random(input_shape[1:])\n",
    "        self.W = K.variable(initial_weight_value)\n",
    "        self.trainable_weights = [self.W]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return x * self.W\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "    return merge([input, residual], mode='sum')\n",
    "\n",
    "\n",
    "def _bn_relu_conv(nb_filter, nb_row, nb_col, subsample=(1, 1), bn=False):\n",
    "    def f(input):\n",
    "        if bn:\n",
    "            input = BatchNormalization(mode=0, axis=1)(input)\n",
    "        activation = Activation('relu')(input)\n",
    "        return Convolution2D(nb_filter=nb_filter, nb_row=nb_row, nb_col=nb_col, subsample=subsample, border_mode=\"same\")(activation)\n",
    "    return f\n",
    "\n",
    "\n",
    "def _residual_unit(nb_filter, init_subsample=(1, 1)):\n",
    "    def f(input):\n",
    "        residual = _bn_relu_conv(nb_filter, 3, 3)(input)\n",
    "        residual = _bn_relu_conv(nb_filter, 3, 3)(residual)\n",
    "        return _shortcut(input, residual)\n",
    "    return f\n",
    "\n",
    "\n",
    "def ResUnits(residual_unit, nb_filter, repetations=1):\n",
    "    def f(input):\n",
    "        for i in range(repetations):\n",
    "            init_subsample = (1, 1)\n",
    "            input = residual_unit(nb_filter=nb_filter,\n",
    "                                  init_subsample=init_subsample)(input)\n",
    "        return input\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "\n",
    "def root_mean_square_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
    "\n",
    "# aliases\n",
    "mse = MSE = mean_squared_error\n",
    "# rmse = RMSE = root_mean_square_error\n",
    "\n",
    "\n",
    "def masked_mean_squared_error(y_true, y_pred):\n",
    "    idx = (y_true > 1e-6).nonzero()\n",
    "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
    "\n",
    "\n",
    "def masked_rmse(y_true, y_pred):\n",
    "    return masked_mean_squared_error(y_true, y_pred) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stresnet(c_conf=(3, 2, 32, 32), p_conf=(3, 2, 32, 32), t_conf=(3, 2, 32, 32), external_dim=8, nb_residual_unit=3):\n",
    "    '''\n",
    "    C - Temporal Closeness\n",
    "    P - Period\n",
    "    T - Trend\n",
    "    conf = (len_seq, nb_flow, map_height, map_width)\n",
    "    external_dim\n",
    "    '''\n",
    "\n",
    "    # main input\n",
    "    main_inputs = []\n",
    "    outputs = []\n",
    "    for conf in [c_conf, p_conf, t_conf]:\n",
    "        if conf is not None:\n",
    "            len_seq, nb_flow, map_height, map_width = conf\n",
    "            input = Input(shape=(nb_flow * len_seq, map_height, map_width))\n",
    "            main_inputs.append(input)\n",
    "            # Conv1\n",
    "            conv1 = Convolution2D(\n",
    "                nb_filter=64, nb_row=3, nb_col=3, border_mode=\"same\",input_shape=input.shape)(input)\n",
    "            # [nb_residual_unit] Residual Units\n",
    "            residual_output = ResUnits(_residual_unit, nb_filter=64,\n",
    "                              repetations=nb_residual_unit)(conv1)\n",
    "            # Conv2\n",
    "            activation = Activation('relu')(residual_output)\n",
    "            conv2 = Convolution2D(\n",
    "                nb_filter=nb_flow, nb_row=3, nb_col=3, border_mode=\"same\")(activation)\n",
    "            outputs.append(conv2)\n",
    "\n",
    "    # parameter-matrix-based fusion\n",
    "    if len(outputs) == 1:\n",
    "        main_output = outputs[0]\n",
    "    else:\n",
    "        new_outputs = []\n",
    "        for output in outputs:\n",
    "            new_outputs.append(iLayer()(output))\n",
    "        main_output = merge(new_outputs, mode='sum')\n",
    "\n",
    "    # fusing with external component\n",
    "    if external_dim != None and external_dim > 0:\n",
    "        # external input\n",
    "        external_input = Input(shape=(external_dim,))\n",
    "        main_inputs.append(external_input)\n",
    "        embedding = Dense(output_dim=10)(external_input)\n",
    "        embedding = Activation('relu')(embedding)\n",
    "        h1 = Dense(output_dim=nb_flow * map_height * map_width)(embedding)\n",
    "        activation = Activation('relu')(h1)\n",
    "        external_output = Reshape((nb_flow, map_height, map_width))(activation)\n",
    "        print(main_output.shape)\n",
    "        print(external_output.shape)\n",
    "        main_output = merge([main_output, external_output], mode='sum')\n",
    "    else:\n",
    "        print('external_dim:', external_dim)\n",
    "\n",
    "    main_output = Activation('tanh')(main_output)\n",
    "    model = Model(input=main_inputs, output=main_output)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model(external_dim):\n",
    "    c_conf = (CONFIG.len_closeness, CONFIG.nb_flow, CONFIG.map_height,\n",
    "              CONFIG.map_width) if CONFIG.len_closeness > 0 else None\n",
    "    p_conf = (CONFIG.len_period, CONFIG.nb_flow, CONFIG.map_height,\n",
    "              CONFIG.map_width) if CONFIG.len_period > 0 else None\n",
    "    t_conf = (CONFIG.len_trend, CONFIG.nb_flow, CONFIG.map_height,\n",
    "              CONFIG.map_width) if CONFIG.len_trend > 0 else None\n",
    "\n",
    "    model = stresnet(c_conf=c_conf, p_conf=p_conf, t_conf=t_conf,\n",
    "                     external_dim=external_dim, nb_residual_unit=CONFIG.nb_residual_unit)\n",
    "    adam = Adam(lr=CONFIG.lr)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=[rmse])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(input_shape=(?, 6, 32,..., padding=\"same\", filters=64, kernel_size=(3, 3))`\n",
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding=\"same\", strides=(1, 1), filters=64, kernel_size=(3, 3))`\n",
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:18: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding=\"same\", filters=2, kernel_size=(3, 3))`\n",
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(input_shape=(?, 2, 32,..., padding=\"same\", filters=64, kernel_size=(3, 3))`\n",
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:37: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:44: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=10)`\n",
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=2048)`\n",
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:51: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ac..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2, 32, 32)\n",
      "(?, 2, 32, 32)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 6, 32, 32)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 2, 32, 32)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_7 (InputLayer)             (None, 2, 32, 32)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)               (None, 64, 32, 32)    3520        input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)               (None, 64, 32, 32)    1216        input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)               (None, 64, 32, 32)    1216        input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 64, 32, 32)    0           conv2d_31[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, 64, 32, 32)    0           conv2d_41[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, 64, 32, 32)    0           conv2d_51[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)               (None, 64, 32, 32)    36928       activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)               (None, 64, 32, 32)    36928       activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)               (None, 64, 32, 32)    36928       activation_49[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 64, 32, 32)    0           conv2d_32[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, 64, 32, 32)    0           conv2d_42[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_50 (Activation)       (None, 64, 32, 32)    0           conv2d_52[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)               (None, 64, 32, 32)    36928       activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)               (None, 64, 32, 32)    36928       activation_41[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)               (None, 64, 32, 32)    36928       activation_50[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "merge_15 (Merge)                 (None, 64, 32, 32)    0           conv2d_31[0][0]                  \n",
      "                                                                   conv2d_33[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_19 (Merge)                 (None, 64, 32, 32)    0           conv2d_41[0][0]                  \n",
      "                                                                   conv2d_43[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_23 (Merge)                 (None, 64, 32, 32)    0           conv2d_51[0][0]                  \n",
      "                                                                   conv2d_53[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 64, 32, 32)    0           merge_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, 64, 32, 32)    0           merge_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_51 (Activation)       (None, 64, 32, 32)    0           merge_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)               (None, 64, 32, 32)    36928       activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)               (None, 64, 32, 32)    36928       activation_42[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)               (None, 64, 32, 32)    36928       activation_51[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 64, 32, 32)    0           conv2d_34[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_43 (Activation)       (None, 64, 32, 32)    0           conv2d_44[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_52 (Activation)       (None, 64, 32, 32)    0           conv2d_54[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)               (None, 64, 32, 32)    36928       activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)               (None, 64, 32, 32)    36928       activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)               (None, 64, 32, 32)    36928       activation_52[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "merge_16 (Merge)                 (None, 64, 32, 32)    0           merge_15[0][0]                   \n",
      "                                                                   conv2d_35[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_20 (Merge)                 (None, 64, 32, 32)    0           merge_19[0][0]                   \n",
      "                                                                   conv2d_45[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_24 (Merge)                 (None, 64, 32, 32)    0           merge_23[0][0]                   \n",
      "                                                                   conv2d_55[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 64, 32, 32)    0           merge_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_44 (Activation)       (None, 64, 32, 32)    0           merge_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_53 (Activation)       (None, 64, 32, 32)    0           merge_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)               (None, 64, 32, 32)    36928       activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)               (None, 64, 32, 32)    36928       activation_44[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)               (None, 64, 32, 32)    36928       activation_53[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 64, 32, 32)    0           conv2d_36[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, 64, 32, 32)    0           conv2d_46[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_54 (Activation)       (None, 64, 32, 32)    0           conv2d_56[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)               (None, 64, 32, 32)    36928       activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)               (None, 64, 32, 32)    36928       activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)               (None, 64, 32, 32)    36928       activation_54[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "merge_17 (Merge)                 (None, 64, 32, 32)    0           merge_16[0][0]                   \n",
      "                                                                   conv2d_37[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_21 (Merge)                 (None, 64, 32, 32)    0           merge_20[0][0]                   \n",
      "                                                                   conv2d_47[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_25 (Merge)                 (None, 64, 32, 32)    0           merge_24[0][0]                   \n",
      "                                                                   conv2d_57[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 64, 32, 32)    0           merge_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, 64, 32, 32)    0           merge_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_55 (Activation)       (None, 64, 32, 32)    0           merge_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)               (None, 64, 32, 32)    36928       activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)               (None, 64, 32, 32)    36928       activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)               (None, 64, 32, 32)    36928       activation_55[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 64, 32, 32)    0           conv2d_38[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, 64, 32, 32)    0           conv2d_48[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_56 (Activation)       (None, 64, 32, 32)    0           conv2d_58[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)               (None, 64, 32, 32)    36928       activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)               (None, 64, 32, 32)    36928       activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)               (None, 64, 32, 32)    36928       activation_56[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 28)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_18 (Merge)                 (None, 64, 32, 32)    0           merge_17[0][0]                   \n",
      "                                                                   conv2d_39[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_22 (Merge)                 (None, 64, 32, 32)    0           merge_21[0][0]                   \n",
      "                                                                   conv2d_49[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_26 (Merge)                 (None, 64, 32, 32)    0           merge_25[0][0]                   \n",
      "                                                                   conv2d_59[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            290         input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 64, 32, 32)    0           merge_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, 64, 32, 32)    0           merge_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_57 (Activation)       (None, 64, 32, 32)    0           merge_26[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_58 (Activation)       (None, 10)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)               (None, 2, 32, 32)     1154        activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)               (None, 2, 32, 32)     1154        activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)               (None, 2, 32, 32)     1154        activation_57[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2048)          22528       activation_58[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "i_layer_4 (iLayer)               (None, 2, 32, 32)     2048        conv2d_40[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "i_layer_5 (iLayer)               (None, 2, 32, 32)     2048        conv2d_50[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "i_layer_6 (iLayer)               (None, 2, 32, 32)     2048        conv2d_60[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_59 (Activation)       (None, 2048)          0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_27 (Merge)                 (None, 2, 32, 32)     0           i_layer_4[0][0]                  \n",
      "                                                                   i_layer_5[0][0]                  \n",
      "                                                                   i_layer_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 2, 32, 32)     0           activation_59[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "merge_28 (Merge)                 (None, 2, 32, 32)     0           merge_27[0][0]                   \n",
      "                                                                   reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_60 (Activation)       (None, 2, 32, 32)     0           merge_28[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 924,648\n",
      "Trainable params: 924,648\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"compiling model...\")\n",
    "model = build_model(external_dim)\n",
    "plot(model, to_file='ST-ResNet.png', show_shapes=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xugeng/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12355 samples, validate on 1373 samples\n",
      "Epoch 1/500\n",
      "   64/12355 [..............................] - ETA: 4951s - loss: 0.4391 - rmse: 0.6490"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-9412a1519983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_rmse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.history.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparams_name = 'c{}.p{}.t{}.resunit{}.lr{}'.format(CONFIG.len_closeness, CONFIG.len_period, CONFIG.len_trend, CONFIG.nb_residual_unit, CONFIG.lr)\n",
    "fname_param = os.path.join('../model', '{}.best.h5'.format(hyperparams_name))\n",
    "early_stopping = EarlyStopping(monitor='val_rmse', patience=5, mode='min')\n",
    "model_checkpoint = ModelCheckpoint(fname_param, monitor='val_rmse', verbose=0, save_best_only=True, mode='min')\n",
    "print(\"training model...\")\n",
    "history = model.fit(X_train, Y_train,\\\n",
    "                        nb_epoch=CONFIG.nb_epoch,\\\n",
    "                        batch_size=CONFIG.batch_size,\\\n",
    "                        validation_split=0.1,\\\n",
    "                        callbacks=[early_stopping, model_checkpoint],\\\n",
    "                        verbose=1)\n",
    "model.save_weights(os.path.join('../model', '{}.h5'.format(hyperparams_name)), overwrite=True)\n",
    "pickle.dump((history.history), open(os.path.join('../result', '{}.history.pkl'.format(hyperparams_name)), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('evaluating using the model that has the best loss on the valid set')\n",
    "model.load_weights(fname_param)\n",
    "score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
    "print('Train score: %.6f rmse (norm): %.6f rmse (real): %.6f' % (score[0], score[1], score[1] * (mmn._max - mmn._min) / 2. * m_factor))\n",
    "score = model.evaluate(X_test, Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
    "print('Test score: %.6f rmse (norm): %.6f rmse (real): %.6f' % (score[0], score[1], score[1] * (mmn._max - mmn._min) / 2. * m_factor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
